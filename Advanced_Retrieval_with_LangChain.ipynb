{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraglahon16/AI-Engineering/blob/main/Advanced_Retrieval_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Parent Document Retrieval\n",
        "- Self-Query Retrieval\n",
        "- Time-weighted Retrieval\n",
        "- Contextual Compression Retrieval\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "> NOTE: We'll provide a separate notebook for the evaluation of these methods which will be available in the YouTube comments"
      ],
      "metadata": {
        "id": "e-IqJAMkwnCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:\n",
        "\n",
        "Installing required libraries!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ],
      "metadata": {
        "id": "3xes8oT-xHN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkgFAXWVW3wm",
        "outputId": "fd6b6729-f9fa-4841-a4e4-77ad728f983f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.5/814.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m854.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment), and require [Lark](https://github.com/lark-parser/lark) for our Self-Query retriever."
      ],
      "metadata": {
        "id": "dKqYM4Eoxcov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU qdrant-client lark"
      ],
      "metadata": {
        "id": "s6xav5CxYnML",
        "outputId": "377dc6ed-f31a-4a31-b86e-97eedc595ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ],
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "c3233e53-a617-41e1-ca9f-b74b9461064a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "1237abb9-2ae6-42b6-aec4-9a6d5ece9368"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohere API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ],
      "metadata": {
        "id": "mw304iAFyRtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ],
      "metadata": {
        "id": "xXKHcZmKzDwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "a56b73f2-49db-45c8-f7b0-d7b225d7e1a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-10 17:39:40--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: ‘john_wick_1.csv’\n",
            "\n",
            "\rjohn_wick_1.csv       0%[                    ]       0  --.-KB/s               \rjohn_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-04-10 17:39:40 (32.7 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
            "\n",
            "--2024-04-10 17:39:40--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_2.csv’\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-04-10 17:39:40 (19.8 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
            "\n",
            "--2024-04-10 17:39:40--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_3.csv’\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-04-10 17:39:41 (22.0 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
            "\n",
            "--2024-04-10 17:39:41--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: ‘john_wick_4.csv’\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-10 17:39:41 (35.4 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ],
      "metadata": {
        "id": "A92NC2QZzCsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ],
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ],
      "metadata": {
        "id": "9gQphb6y0C0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e691e5ba-b0aa-4c34-a0cd-a443962d8d26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\", metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2024, 4, 7, 17, 39, 41, 693983)})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QDrant VectorStore\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ],
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ],
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ],
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive rertriever will simple look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ],
      "metadata": {
        "id": "NEH7X5Ai08FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ],
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ],
      "metadata": {
        "id": "MbBhyQjz06dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ],
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ],
      "metadata": {
        "id": "BlRzpb231GGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ],
      "metadata": {
        "id": "mg3QRGzA1M2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ],
      "metadata": {
        "id": "izKujhNb1ZG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LI-5ueEddku9",
        "outputId": "c99dc872-f20a-49b4-d022-96483212ceb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the context provided, it seems that people generally liked John Wick. The majority of the reviews praise the movie for its action sequences, Keanu Reeves' performance, and overall entertainment value. Some reviewers even mention that it is one of the best action films they have seen in recent years.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "43zdcdUydtXh",
        "outputId": "4b2b4088-05be-453c-8ac3-5ca12cbab270"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, there is a review with a rating of 10. Here is the URL to that review: '/review/rw4854296/?ref_=tt_urv'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "37eb1d33-2724-47a7-fd5f-280c860897e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In \"John Wick,\" an ex-hitman comes out of retirement to seek vengeance against the gangsters who killed his dog and took everything from him. This leads to a series of violent encounters and action-packed sequences as John Wick faces off against various enemies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ],
      "metadata": {
        "id": "jsbfQmbr1leg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ],
      "metadata": {
        "id": "EDEawBf_d_3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ],
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ],
      "metadata": {
        "id": "oOpXfVUH3gL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ],
      "metadata": {
        "id": "rzFc-_9HlGQ-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ],
      "metadata": {
        "id": "sf_g95FA3s6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ],
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ],
      "metadata": {
        "id": "KoYmSWfE32Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ],
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ],
      "metadata": {
        "id": "bI7Tip1335rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give it a whirl!"
      ],
      "metadata": {
        "id": "jNolUVQb4Apt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TXB5i89Zly5W",
        "outputId": "b008d11b-3061-40e8-e92b-db3cb3d5596c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some people did like John Wick, while others did not.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "dbf6ed5c-aa00-4fd3-e2a8-43fc0e6d23a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review: /review/rw4854296/?ref_=tt_urv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ZqARszGzvGcG",
        "outputId": "a12480e6-3c32-484f-8f35-1db7402d968c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In John Wick, the main character, played by Keanu Reeves, is a retired assassin who comes out of retirement seeking revenge when someone kills his dog and steals his car. This sets off a chain of events that leads him to pay off an old debt by helping take over the Assassin's Guild in various locations like Italy, Canada, and Manhattan, where he ends up killing a lot of assassins.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ],
      "metadata": {
        "id": "B41cj42s4DPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Query\n",
        "\n",
        "We have another problem with retrieval - what if people are asking questions based on the metadata, instead of the contained context?\n",
        "\n",
        "That's where Self-Query shines!\n",
        "\n",
        "It not only lets us search our vectorstore in the traditional way (semantic similarity), but we can also filter our vectorstore intelligently based on the provided metadata!\n",
        "\n",
        "Let's start by describing the relevant metadata, and what type of data it is.\n",
        "\n",
        "> NOTE: As always, there's the potential for the natural language description to be leveraged by an LLM - so take care to write informative and descriptive...descriptions!"
      ],
      "metadata": {
        "id": "L7eVqIuHndxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"Movie_Title\",\n",
        "        description=\"The title of the movie\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"Review_Date\",\n",
        "        description=\"The date of the review\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"Review_Title\",\n",
        "        description=\"The title of the review\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"Review_Url\",\n",
        "        description=\"The URL of the review\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"Author\",\n",
        "        description=\"The author of the review\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"Rating\",\n",
        "        description=\"A 1 to 10 rating for the movie\",\n",
        "        type=\"integer\",\n",
        "    )\n",
        "]\n",
        "\n",
        "document_content_desription = \"A review of the movie.\"\n",
        "\n",
        "self_query_retriever = SelfQueryRetriever.from_llm(\n",
        "    ChatOpenAI(temperature=0),\n",
        "    vectorstore,\n",
        "    document_content_desription,\n",
        "    metadata_field_info,\n",
        ")"
      ],
      "metadata": {
        "id": "yXqajPS7ns9b"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's really all we need to do!\n",
        "\n",
        "Let's create our chain, and see how this does!"
      ],
      "metadata": {
        "id": "NRx_xloH5YZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | self_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "00jOVo98q1lg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QJcBNTt8q9Zu",
        "outputId": "23172042-e789-4d6e-b638-edb44fd17241"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "RFFr8WUzrB-i",
        "outputId": "af93e0e1-8669-4f52-a376-2a38c5157101"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, there are reviews with a rating of 10. Here are the URLs to those reviews:\\n1. Review URL: /review/rw7582036/?ref_=tt_urv\\n2. Review URL: /review/rw4860603/?ref_=tt_urv\\n3. Review URL: /review/rw4854296/?ref_=tt_urv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "EfbiNk6mvA4a",
        "outputId": "c921ad39-dda1-4d18-a53a-b878a15ada84"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to track down the gangsters that killed his dog and took everything from him. He seeks vengeance and unleashes a maelstrom of destruction against those who try to chase him. It is a story of revenge and intense action.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see - the question concerning specific metadata was answered much better than the previous methods!"
      ],
      "metadata": {
        "id": "mRQDaz3z5gha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time-weighted Vector Store\n",
        "\n",
        "Next up, we have a method that leverages temporal data to more prefer the most recently accessed documents in our VectorStore.\n",
        "\n",
        "> NOTE: In this example we've created a relationship between when the movie came out, and when the chunks were accessed, this is simply for illustrative purposes."
      ],
      "metadata": {
        "id": "vdey4mLb5lTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we'll create a new QDrant vectorstore using the in-memory client."
      ],
      "metadata": {
        "id": "Kl-8EKOj6FrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"time_weighted\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "time_weighted_vectorstore = Qdrant(\n",
        "    collection_name=\"time_weighted\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ],
      "metadata": {
        "id": "kwwA5Uf_usmL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `TimeWeightedVectorStoreRetriever`, you'll notice that we have a parameter called `decay_rate`. This `decay_rate` is used in the following calculation to scale our cosine-similarity scores by when the document was last accessed:\n",
        "\n",
        "$semantic\\_similarity + (1.0 - decay\\_rate) ^ {hours\\_passed}$\n",
        "\n",
        "As you can see, the decay_rate will be set between $(0-1)$, where $0$ will mean we're never decaying our semantic similarity scores, and $1$ will mean we're also never decaying our semantic similarity scores.\n",
        "\n",
        "As we increase the decay rate from $0 -> 1$ we're going to see items more and more aggresively decaying as time moves along.\n",
        "\n",
        "So a \"low\" decay rate (closer to 0) will lead to items being less penalized for being accessed less-recently - and a \"high\" decay rate (closer to 1) will lead to items being more penalized for being accessed less-recently."
      ],
      "metadata": {
        "id": "PaC0jWzW6NVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_weighted_retriever = TimeWeightedVectorStoreRetriever(\n",
        "    vectorstore=time_weighted_vectorstore, decay_rate=0.6, k=2,\n",
        ")\n",
        "\n",
        "time_weighted_retriever.add_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGbN1C7VtmQ4",
        "outputId": "19709f47-63ed-4fa3-9990-6a0cf4be37d8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f3cffbdaedbd45b9b58a7c064ebba7ae',\n",
              " '0821312349704a208180f427f8bb2f30',\n",
              " '61497bad3a2441f3bcafcd0eaa852203',\n",
              " '4e9b475967684e5aa9960dcd6a54b18c',\n",
              " 'ffecc041f6d44ebcbccc051da3e62769',\n",
              " '20458f215e814eee87eba2955ebdd638',\n",
              " 'b1c0ec046eb84246a4c003e155df28d3',\n",
              " 'd0833e23b36647ba8ee292b43804d86f',\n",
              " 'b52b2348420c4e69a67ba0d59fe579bc',\n",
              " 'e1a2531bcdda40bc9daa2a2c3a5d65c0',\n",
              " 'd04fd4f37e2144d987a6abcb91c4c0a2',\n",
              " 'a13f1c9239364b27a674224986b361ff',\n",
              " '9f2f4644a2364ad2ace7b62de0e252a6',\n",
              " '7c628d7bd01745049142f5fd99983dba',\n",
              " 'd27d310e98b242078da6233ee26a1035',\n",
              " 'e9649dc1095b4dc68aaac68f9da71c92',\n",
              " 'ad67cc922ed546e0b2faa71f0e453271',\n",
              " '844259a78f3945c69ab8479d0a7ec1ca',\n",
              " 'e38331b5a44c47509f8cff79409f0dba',\n",
              " '5bb881cf43cd4d6fa817626eca974130',\n",
              " '8f103b6dea3549e5bf9109705c9abe62',\n",
              " 'cf1d19141c9b440e8eeddd9f3b8a685c',\n",
              " '970da81de2be454aaef287247a16b844',\n",
              " 'ea15ac41a071481682f44de187051301',\n",
              " 'a4c254032b34437abb7ccd0dce128e2b',\n",
              " '65613ab47e564c188f2fbbc591f9fe31',\n",
              " '51b0c3b87590441790c8951077baff3e',\n",
              " 'f4213382c7d14505b893982ee974c1fc',\n",
              " 'c9368147e3704d35baf7c7366c937dd1',\n",
              " '59b4ddd86f8f41e1bccb56b67d117861',\n",
              " 'eeffaa803a434fc3981a911a095b6eb8',\n",
              " '6e6cda08078c4c1aae47900c49f9982d',\n",
              " '991e0c1830a64c0c866becee89f332b0',\n",
              " '9d5c53f5d7c14abe9366876d6b53a137',\n",
              " '78a9df177bd74c9a94dab6c9b68a7788',\n",
              " '169d2d9bf27f4c149b46aacb917a1474',\n",
              " '3bf7bed407fa4dd2b51f18eeb85d81e8',\n",
              " 'df98aa8521124954afca52777e3c4ab4',\n",
              " '36c606953e874e6f9d544c2bc3727162',\n",
              " '5a559904f7114c2fb814b7fbab9ceb4f',\n",
              " 'b27420338af0455cbd9ed477be115967',\n",
              " '260838106b1f4637946742d824821463',\n",
              " '96ca06ef7ee04374a5c9772a2173da50',\n",
              " '6ff4cf68a3a14998a81acba70555df57',\n",
              " '9e27ad433a8c411db51c7895c7d56836',\n",
              " '7964bcb78d8344f9b5a715cdb233059e',\n",
              " 'ecbe12f2bbf54623bcf96a76bd151004',\n",
              " '9be38154a09449f4940fe47243fcaed3',\n",
              " '3038353158684df586760fd6ba592651',\n",
              " '9e0687c475514f219082c7d415fd7d5e',\n",
              " 'e48a0198df234b7ba541482c2064f266',\n",
              " '97385fcc345242a1bc4295fea80f1218',\n",
              " '67b73ee2c0e1446e8dd5b02e31801430',\n",
              " '0b42c43e9bd64f0e81fd5fde87c08f7c',\n",
              " '2b1cce3505024b368d3d1dd90e57b375',\n",
              " '64c6d39dd6634416bb8e32410426e94e',\n",
              " 'c132b7a1883d41c897e480784ec318a4',\n",
              " '75f2cd34158e416fb1e3a77d0a241e3a',\n",
              " '0d6c99298e4e489595ac19b91114b23c',\n",
              " '8410a7ce23b54b90a8c612ed194d5633',\n",
              " 'fed4a67169e24381a7f8bf846f4fd551',\n",
              " '21e000ae259b4a3d898a6188a3948a6b',\n",
              " 'b17cc17bcf494f50b31f72fa8e108b3a',\n",
              " 'e382aaab1f754632a2a13a2f03c0ee1e',\n",
              " 'c5c2574a897e4ae38ff40af1760f4646',\n",
              " '9048a58e24864e1ab443bfb7faf76228',\n",
              " 'df82f3fbd3234ced9038a433f40bbfe2',\n",
              " '5587896e855949baa66f0719f38b495f',\n",
              " '95985d6e9ed2455f9bbc49db0453d676',\n",
              " 'a7589fb6b8434204bb27fc7dc3e430fa',\n",
              " 'c6889801615143c78122665d52648d39',\n",
              " '800a14bd62a44ca1af6328ba329aca03',\n",
              " '0f7dcc0ab0ff405eb67737190692cff0',\n",
              " '76e7dd39f49843e6867e1a673a23c049',\n",
              " 'e9bbde79b3894b24a8b258879d418694',\n",
              " 'ef16493d890f4beba0469a668be9b5ab',\n",
              " '9109f601e9fc46dcbc895af3e8f4c12e',\n",
              " 'd33dbeff38a84892aabf4616fd9ce8e3',\n",
              " '27d5427902ee427e865080dbebe51054',\n",
              " '87e7aa0b0562425f8c1c8bc8a536c96b',\n",
              " '9377c9a96de9434190d12b3acf56a9d4',\n",
              " '74170cdbf3dc4417970056d29d7ce56c',\n",
              " '802a6f4f0a874661a6cdf803930adb92',\n",
              " '77358779c756454aa27eac0f23de4584',\n",
              " 'aea8365794014e17886bd3da49e76f67',\n",
              " '288585f778994747a60a80b4fa8c3e2f',\n",
              " 'f4407609e79b425fa9e4d9109ed2eb9c',\n",
              " '97f671360823433aa00e86e6bc635b25',\n",
              " 'fba0edb8c2994ac897f69f01fb7392ab',\n",
              " 'd0ba36de1b064a49a7f7c111379382ff',\n",
              " '4527647661eb4ccb87569d5ae5dd507f',\n",
              " '98f4bb64d1b5492fb8883ff384ccf6bf',\n",
              " '4973787328ce4d4cbc7398940aeddf58',\n",
              " '318b7facbed64d16af6dffaeb9249ec6',\n",
              " '07f274fc54e747b5b42243540ebcd6fa',\n",
              " '626d101b9c98448490ee44a6200147d8',\n",
              " '18ed9b86d46f46d0907eb22ef31efe2e',\n",
              " '6aec8dce6fba41239c01d74f9928b673',\n",
              " 'e6d00d698a2d44409d01982c0c73b417',\n",
              " 'ed38ac199d14461fa360525ee57b8757']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our same RAG chain that we've been using so far!"
      ],
      "metadata": {
        "id": "P4lJVH708Xf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_weighted_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | time_weighted_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "lLdJW2vSu2UP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this iteration!"
      ],
      "metadata": {
        "id": "15p5N_cV8uIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_weighted_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EhjeB3ctuehR",
        "outputId": "9a83a395-6cf7-4523-883a-d4ec731857c0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick 4 based on the reviews provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_weighted_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qNxNQizAvSYz",
        "outputId": "3c1dea58-de90-43b5-f1e9-f68ebe4e7e26"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, there are no reviews with a rating of 10 in the provided context.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_weighted_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YIRhGCXUvVC3",
        "outputId": "7488fcd4-f433-42a8-a37e-b196dbf6b7e2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In John Wick 4, there is a lot of action involving gunfights and people being thrown down stairs. It seems to be very intense and action-packed, with some reviewers mentioning that it is even more intense than the previous movies in the franchise.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we're heavily biasing the more \"recently accessed\" documents (John Wick 4, in this case)."
      ],
      "metadata": {
        "id": "U50f7UbF8wh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual Compression\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ],
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our chain again, and see how this does!"
      ],
      "metadata": {
        "id": "_TA9RB2x-j7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V3iGpokswcBb",
        "outputId": "5d474f34-ca03-4f5c-8182-db1ce5a465d0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "e6b618cd-da28-4639-992f-129a75b26b5a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n- Review: A Masterpiece & Brilliant Sequel\\n- URL: [Click here to view the review](/review/rw4854296/?ref_=tt_urv)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "zn1EqaGqweXN",
        "outputId": "98d1cab7-fe97-4f3c-d200-97ddd38a085b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In John Wick, after resolving his issues with the Russian mafia, John Wick returns home, but soon the mobster Santino D'Antonio visits him with a marker and asks for his help. John Wick refuses, so Santino blows up his house. Wick meets with Winston, the owner of the Continental hotel, who tells him he must honor the marker and kill Santino's sister in Rome. After completing the task, Santino puts a contract on Wick, leading to a showdown between them.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, it's hard to qualitatively assess this solution - but it \"seems\" better!"
      ],
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      }
    }
  ]
}