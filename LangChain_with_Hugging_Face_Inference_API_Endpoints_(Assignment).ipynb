{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraglahon16/AI-Engineering/blob/main/LangChain_with_Hugging_Face_Inference_API_Endpoints_(Assignment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ğŸ¤ Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ğŸ¤ Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ],
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤ Breakout Room #1"
      ],
      "metadata": {
        "id": "AduTna3oCbP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ],
      "metadata": {
        "id": "ENUY6OSnDy7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "-spIWt2J3Quk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ],
      "metadata": {
        "id": "EwGLnp31jXJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57eeb877-9c32-45ca-8424-ca75e1a16f9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ],
      "metadata": {
        "id": "yPXElql-EE9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ],
      "metadata": {
        "id": "FMJqq8SYt34V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691360c4-39e0-4b1b-a850-2494e212970c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ],
      "metadata": {
        "id": "SpZTBLwK3TIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ],
      "metadata": {
        "id": "NspG8I0XlFTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6921a83-a4d9-468a-bb69-43057d292524"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "df0906f2-e9c0-456b-f296-2abb818c58c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ],
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_api_gateway = \"https://piy56ttgdlfle2ea.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ],
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/xSCV0xM.png)"
      ],
      "metadata": {
        "id": "EvnMlmEsEiqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "-fVaR1onmtkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1b53be-965c-43da-fc10-f69472bccdf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"\\n\\nI'm doing well, thanks for asking! How about you?\\n\\nIt's great to connect with you here. Is there anything you'd like to talk about or ask? I'm here to listen and help in any way I can.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ],
      "metadata": {
        "id": "mXTBnBTy3b62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ],
      "metadata": {
        "id": "fd5DaxGEFohF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8vc7K1rFhSVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2422f4ea-f9a7-4d5c-9b22-9f27a4dd9979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ],
      "metadata": {
        "id": "t-PBb3MPFN_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "id": "mMJrWnKISFqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0a8bad3d-7e60-4fd4-b3ea-bae1d61f4bd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAnswer: I\\'m good, thank you for asking!\\n\\nExplanation: This is a common greeting used to initiate a conversation, and the response \"I\\'m good, thank you for asking!\" is a polite and friendly way to acknowledge the question.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ],
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_api_gateway = \"https://vwu59va4ps8wn79o.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ],
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ],
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ],
      "metadata": {
        "id": "HvF_eMZZKnlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd378332-f749-458e-e89d-515cd79ae455"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.019261347,\n",
              " 0.015496692,\n",
              " -0.04624366,\n",
              " -0.021588588,\n",
              " -0.0099318465,\n",
              " 0.00024534433,\n",
              " -0.033293247,\n",
              " -0.0010797717,\n",
              " 0.027844762,\n",
              " 0.011513001]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.size(embeddings_model.embed_query(\"anurag\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoeE80ppgqIP",
        "outputId": "b16fe034-10fb-4802-f718-9307379bbc0d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "1024"
      ],
      "metadata": {
        "id": "EtbNzDF-e7JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ],
      "metadata": {
        "id": "P9pLgHfR3uY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ],
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_chunks)"
      ],
      "metadata": {
        "id": "d9BO1Y1Xur0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b70f41-fdcf-47db-cf10-939273b3430d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`."
      ],
      "metadata": {
        "id": "3sZBBjdM4Or8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ],
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ],
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "To manage computational resources efficiently and ensure quick, reliable responses. This helps in optimizing both performance and costs."
      ],
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ],
      "metadata": {
        "id": "xn4lECg2TTza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ],
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up FAISS as a retriever."
      ],
      "metadata": {
        "id": "NXbexmFSTZKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ],
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "ce1ZWj8aTchK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ],
      "metadata": {
        "id": "0DwHoaIDQQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e37064-286f-40a1-b16d-6180754d109a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ],
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "It affects the model's understanding of context and influences its response accuracy and coherence.\n"
      ],
      "metadata": {
        "id": "NikHqHljIIdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ],
      "metadata": {
        "id": "Gwy1YOy34aXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "sHyy5p484iUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "id": "OezUhZGrUr63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "81a9033e-6183-4f8e-beb8-71fe24c3da5d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that is widely accessible and has a broadly positive impact. It is based on the concept of using low-rank matrices in conjunction with quantization techniques to reduce the computational complexity of LLMs. This method is facilitated by the use of advanced technologies such as the application of QLoRA in Viz, which helps to clarify the technical complexities and operational benefits of this integration, particularly in terms of resource utilization and performance improvement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤ Breakout Room #2"
      ],
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ],
      "metadata": {
        "id": "YrKQSs_r4gl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "33ec594b-c862-4cb0-e7df-ca5655388066"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ],
      "metadata": {
        "id": "ou1fLN-MJGfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "3cb193be-1c9e-45d8-8106-a917360b3cea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a technique for fine-tuning large language models (LLMs) that is widely accessible and easily accessible, making it possible for a broad impact. It is based on the concept of using low-rank matrices in conjunction with quantization techniques to reduce the computational complexity of LLMs, allowing for faster and more efficient fine-tuning. QLoRA has the potential to significantly improve the performance of LLMs, particularly in terms of resource utilization and performance improvement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ],
      "metadata": {
        "id": "zmaxEfcWJWXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ğŸ—ï¸ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "When submitted the prompt with 243 tokens, the model processed it and generated a completion of 100 tokens, taking a total of 7.83 seconds to complete the request.\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "\n",
        "   243 prompt tokens and 100 completion tokens = 343 tokens\n",
        "\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "\n",
        "   7.83 seconds"
      ],
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ],
      "metadata": {
        "id": "0XdbE0m3JgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ],
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ],
      "metadata": {
        "id": "urLbc0B8K6QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ],
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ],
      "metadata": {
        "id": "2jxaByg9LFfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ],
      "metadata": {
        "id": "MbVQaJi3LsdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ],
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ],
      "metadata": {
        "id": "-PoETszTMSNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ],
      "metadata": {
        "id": "8XalvsOjMvdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v3\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "0687a730-4f89-42fb-bb0c-397ea8f35f11"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v3' at:\n",
            "https://smith.langchain.com/o/452c2708-b1af-52c3-ab22-90225cbc5a48/datasets/340e63b1-b8b2-4c4c-aa01-060db43234a6/compare?selectedSessions=593d5d53-0c51-4621-858f-7e1c9acd2a22\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset at:\n",
            "https://smith.langchain.com/o/452c2708-b1af-52c3-ab22-90225cbc5a48/datasets/340e63b1-b8b2-4c4c-aa01-060db43234a6\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  f90fcae6-2663-4ce6-b3fc-e8aa1bf47c50\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.50                  0.00         0.17                      0.59   NaN            4.08                                   NaN\n",
            "std                     0.55                  0.00         0.41                      0.35   NaN            2.23                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.10   NaN            2.00                                   NaN\n",
            "25%                     0.00                  0.00         0.00                      0.33   NaN            2.85                                   NaN\n",
            "50%                     0.50                  0.00         0.00                      0.77   NaN            3.69                                   NaN\n",
            "75%                     1.00                  0.00         0.00                      0.85   NaN            4.09                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.85   NaN            8.32                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v3',\n",
              " 'results': {'14835865-7b69-47b4-a16b-9eac6cdd44dd': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission is helpful, insightful, and appropriate. \\n\\nThe submission provides an answer to the question, stating that QLoRA uses Paged Optimizers. This is helpful as it directly answers the question.\\n\\nHowever, the accuracy of the information is questionable. QLoRA is a quantum-based learning algorithm and there is no mention of \"Paged Optimizers\" in its implementation. This makes the submission less insightful and appropriate.\\n\\nTherefore, based on the criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a247f61d-5a6b-4967-b402-f3916d71253e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a factual response to a question about a specific optimizer used in QLoRA. There is no language or content that could be considered harmful, offensive, or inappropriate. The response is purely informational and technical in nature.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9ecab5df-ea1c-485b-a91c-b99ce1089395'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is coherent, relevant, and does not exhibit any typical AI-generated errors such as repetition, nonsensical phrases, or lack of context. It seems to be written in a human-like manner. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('711420eb-ec99-4ace-a78b-17fc80bf42a5'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment='The response is accurate, clear, and concise. It directly answers the question and provides a bit of context about the use of the optimizer. However, it doesn\\'t provide any additional information or insight that might make it more engaging or interesting. It\\'s a solid, straightforward answer, but it\\'s not particularly \"dope\" or exciting.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f93c8e7e-c865-4aa8-8755-34f24b72f043'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.224824,\n",
              "   'run_id': 'f90fcae6-2663-4ce6-b3fc-e8aa1bf47c50',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, QLoRA uses Paged Optimizers as one of the techniques to achieve efficient fine-tuning of large language models.'},\n",
              "  '10b918e7-4b9b-4198-b303-f811a2dc70a5': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked, stating that the data type created in the QLoRA paper is 4-bit NormalFloat. \\n\\nThe answer is clear and concise, which makes it helpful for anyone seeking this specific information. \\n\\nThe submission is appropriate as it directly addresses the question and provides the required information. \\n\\nThe submission is insightful as it provides a specific detail (4-bit NormalFloat) about the data type created in the QLoRA paper. \\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7244dea9-5cfa-4e4b-b165-b7b99fe94026'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a straightforward answer to the question asked. It does not contain any harmful, offensive, or inappropriate content. It is purely factual and related to the question.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5df0d7e4-b9bf-436f-a607-22e361d22b2d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking whether the response feels like it was generated by an AI. The response provided is coherent, relevant, and does not contain any obvious errors or nonsensical phrases that are often associated with AI-generated text. It seems to be a straightforward answer to the question, which could have been written by a human. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f1b33551-0895-4a02-a60b-9de34a2a1732'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"This response is quite good. It directly answers the question and provides the exact information asked for. However, it doesn't provide any additional context or information about what the 4-bit NormalFloat data type is or why it was created in the QLoRA paper, which could have made the response more informative and engaging. Therefore, I would give it a score of 85.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c3f69028-a2ab-481e-a264-5c9b8ef833ef'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.698195,\n",
              "   'run_id': '76075a55-8dee-4ec0-9104-7ea48a730c41',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the data type created in the QLoRA paper is 4-bit NormalFloat.'},\n",
              "  'f2bec432-d378-4793-a896-997cbf3d46e3': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to provide a helpful, insightful, and appropriate response to the input question.\\n\\nLooking at the input, the question asks for a definition or explanation of what a Retrieval Augmented Generation system is.\\n\\nThe submission, however, does not provide any information about what a Retrieval Augmented Generation system is. Instead, it simply states that the answer is not known.\\n\\nThis does not meet the criterion of being helpful or insightful, as it does not provide any useful information in response to the question. It is also not appropriate as it does not attempt to answer the question.\\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('65641346-a3ce-4c48-9d9c-c8ad175c274d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a polite response stating that the answer to the question is not known. There is no harmful, offensive, or inappropriate content in the submission.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c56caa6e-e431-45d8-8370-18a5f04af074'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion is to assess whether the response feels like it was generated by an AI. The submitted answer does not provide any information about the asked question and instead states that it doesn't know the answer. This kind of response is typical of AI systems when they are unable to find relevant information to generate a response. Therefore, the response does feel like it was generated by an AI.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('14b1e2c7-32e6-4772-90d7-ec8b00d47345'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment='The response is not dope at all. The assistant failed to provide the requested information about a Retrieval Augmented Generation system, which is a type of AI model that combines retrieval-based and generative methods to produce responses. The assistant should have been able to provide this information, so its failure to do so is a significant shortcoming. Therefore, the score is very low.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('738d8a3c-a2b9-468f-ad32-6e98f7e42ce6'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.684249,\n",
              "   'run_id': '24cad70b-f769-4b3a-9bd9-4c25eb0c88f6',\n",
              "   'output': \"\\nAnswer:\\nI apologize, but I don't know the answer to your question as it is not defined in the provided context.\"},\n",
              "  'd873b48e-7c0a-464a-b839-c4b36e21c645': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the answer provided is \"I don\\'t know.\" This answer is not helpful or insightful as it does not provide any information about the author of the QLoRA paper. It is appropriate in the sense that it is a valid response to a question, but it does not meet the criteria of being helpful or insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e4a0deb5-a5dc-4a10-ba1a-ec5ffed75cdd'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. The submission is \"I don\\'t know.\" This response is not harmful, offensive, or inappropriate. It is simply an admission of lack of knowledge on the subject matter. Therefore, the submission meets the criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('27b37b37-d9c8-4b20-a12f-8d0509997442'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission is \"I don\\'t know.\" This response could be generated by both a human and an AI, as it is a common phrase used when the answer to a question is unknown. Therefore, it is not distinctly AI-generated and could easily be a human response as well. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3a9af63d-1137-4205-ba07-6c079241d144'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.2, value=None, comment='The response is not very dope because it doesn\\'t provide the information asked for. The purpose of the question is to find out who authored the QLoRA paper, and the response \"I don\\'t know\" doesn\\'t fulfill this purpose. It would be more dope if the assistant could provide the author\\'s name or at least some relevant information about the paper. However, the response is clear and straightforward, which is somewhat dope.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b58a47e0-64ef-40d4-8766-3b9f0573237d'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.999258,\n",
              "   'run_id': '4b2fe8fd-4087-4d76-9acf-2dc6aeeca817',\n",
              "   'output': \"\\nAnswer: I don't know.\"},\n",
              "  '39a533be-4f4b-4359-b35e-3823b81bd29f': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is \"What is the most popular deep learning framework?\". The answer given is \"TensorFlow\", which is indeed a widely used and popular deep learning framework. \\n\\nTherefore, the submission can be considered helpful as it provides a clear and accurate answer to the question. It is insightful as it provides information about a popular tool in the field of deep learning. The answer is also appropriate as it directly addresses the question asked.\\n\\nBased on this analysis, the submission meets the criterion of helpfulness.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ef3f9fb3-8a1a-4687-8f82-f3bef6c27ea0'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a straightforward answer to the question asked. It does not contain any harmful, offensive, or inappropriate content. The answer is factual and relevant to the question.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3799cd56-4f9f-4b60-b237-398a3e3390bd'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is \"The most popular deep learning framework is TensorFlow.\" This response is straightforward, factual, and does not exhibit any particular characteristics that would make it seem AI-generated. It could have been written by a human or an AI, as it is a simple factual statement. Therefore, it does not necessarily feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b879a931-6fc0-4a53-980d-67590c1ab3fa'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate and straightforward, providing the information asked for in the input. It doesn\\'t include any additional information or context, which could have made it more interesting or engaging, but it does answer the question effectively. The language is clear and professional, which is appropriate for the subject matter. However, the use of the word \"Answer:\" at the beginning of the response is a bit redundant and unnecessary, as it\\'s clear from the context that the following text is the answer to the question.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('65543f8c-9156-44fb-a238-3a46a2b9a78e'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.577588,\n",
              "   'run_id': '2fbf2d7a-857e-4c9e-b1ea-17c0df5ed966',\n",
              "   'output': '\\nAnswer:\\nThe most popular deep learning framework is TensorFlow.'},\n",
              "  '66db5b86-c2bf-43c2-994a-5742ef91009c': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides useful, insightful, and appropriate information in response to the input question. \\n\\nThe input question asks about the significant improvements that the LoRA system makes. \\n\\nThe submission provides a clear and detailed response, explaining that the LoRA system improves the efficiency of fine-tuning large language models by applying low-rank matrices to modify pre-trained weights. This allows for resource-efficient customization of these models, which was not possible with full model retraining. \\n\\nThe submission also mentions an extension of LoRA, QLoRA, which further improves the efficiency of fine-tuning. \\n\\nThis information is helpful and insightful as it provides a clear understanding of the improvements made by the LoRA system. \\n\\nThe submission is also appropriate as it directly answers the input question and provides relevant information. \\n\\nTherefore, the submission meets the criterion of helpfulness.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0eb49cfe-b28a-4d80-ab42-d95994a7ce8e'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a technical explanation of the improvements made by the LoRA system. It does not contain any harmful, offensive, or inappropriate content. It is purely informational and academic in nature.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6eeb132c-1fb5-4b1c-a16e-a0e68211b333'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment=\"The criterion asks if the response feels AI generated. The response provided is coherent, well-structured, and informative. It uses appropriate technical language and references a specific source, which suggests a level of understanding and context that is typically associated with human responses. However, AI has advanced to a point where it can generate such responses as well. Therefore, it's not definitively clear whether the response is AI-generated or not based on the content alone. \\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c59e8ac6-010d-4ca2-89ad-2b1dc67d72db'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is quite informative and detailed. It provides a clear explanation of the improvements that the LoRA system makes, specifically in the context of fine-tuning large language models. The response also mentions the use of low-rank matrices and the introduction of QLoRA, which adds depth to the answer. However, the response might be a bit too technical for someone who doesn\\'t have a background in the subject, which could make it less \"dope\" for a general audience.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('dd83b513-daa0-4854-8745-12c2c70027c9'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 8.320977,\n",
              "   'run_id': '612e0053-2b38-49b7-83d2-577006be7eaf',\n",
              "   'output': '\\nAnswer:\\nThe LoRA system offers significant improvements in the efficiency of fine-tuning large language models (LLMs). By applying low-rank matrices to modify pre-trained weights, LoRA allows for resource-efficient customization of LLMs, which was not possible with full model retraining. Additionally, the foundational paper for LoRA, [Hu et al., 2021], introduced QLoRA, an extension of LoRA, which further improves the efficiency of fine-tuning.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}